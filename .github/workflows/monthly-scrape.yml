name: Monthly Council Meeting Scraper

on:
  # Run on the 1st of every month at 2 AM UTC
  schedule:
    - cron: '0 2 1 * *'
  
  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:
    inputs:
      max_documents:
        description: 'Maximum number of documents to process'
        required: false
        default: '10'
        type: string

jobs:
  scrape-and-summarize:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install Chrome and ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
      
      - name: Install Python dependencies
        run: |
          cd flask-backend
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper pipeline
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          cd flask-backend/scraper
          python orchestrator.py --max ${{ github.event.inputs.max_documents || '10' }}
      
      - name: Summary
        run: |
          echo "âœ… Scraper completed successfully"
          echo "Check your Supabase database for new summaries"